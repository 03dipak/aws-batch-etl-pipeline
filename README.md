# Customer 360 Analytics - AWS Glue ETL Pipeline

This project implements a batch ETL data pipeline for Customer 360 analytics using **Apache Spark on AWS Glue**. It reads data from **Amazon RDS** and **DynamoDB**, performs transformations, and stores the output in **Amazon S3**. The infrastructure and deployment are fully automated using **Boto3 scripts** and **GitHub Actions CI/CD**.

## ğŸ“ Project Structure
<pre><code>
ğŸ“¦ repo-root/
â”£ ğŸ“‚glue_etl_pipeline                 â†’ Modular ETL scripts for AWS Glue
â”ƒ â”£ ğŸ“œ__init__.py                     â†’ Makes directory a Python package
â”ƒ â”£ ğŸ“œchurn_prediction.py             â†’ Glue job for churn prediction
â”ƒ â”£ ğŸ“œomni_channel_engagement.py      â†’ Engagement analysis ETL
â”ƒ â”£ ğŸ“œfraud_detection.py              â†’ Fraud detection ETL
â”ƒ â”£ ğŸ“œpricing_trends.py               â†’ Price trend analysis
â”ƒ â”£ ğŸ“œpurchase_behavior.py            â†’ Customer purchase behavior logic
â”ƒ â”£ ğŸ“œglue_config.py                  â†’ Constants/config for Glue jobs
â”ƒ â”£ ğŸ“œutils.py                        â†’ Utility functions for Glue jobs
â”ƒ â”— ğŸ“œlogging.py                      â†’ Centralized logging
â”£ ğŸ“‚glue_upload_script                â†’ S3-uploadable Glue job scripts
â”ƒ â”£ ğŸ“œpurchase_behavior.py
â”ƒ â”£ ğŸ“œchurn_prediction.py
â”ƒ â”£ ğŸ“œomni_channel_engagement.py
â”ƒ â”£ ğŸ“œfraud_detection.py
â”ƒ â”— ğŸ“œpricing_trends.py
â”£ ğŸ“‚infrastructure                    â†’ AWS resource provisioning (Boto3-based)
â”ƒ â”£ ğŸ“œcreate_glue_jobs.py
â”ƒ â”£ ğŸ“œcreate_glue_crawlers.py
â”ƒ â”£ ğŸ“œcreate_glue_workflows.py
â”ƒ â”£ ğŸ“œcreate_s3_buckets.py
â”ƒ â”£ ğŸ“œiam_roles.py
â”ƒ â”— ğŸ“œdeploy_infrastructure.py
â”£ ğŸ“‚scripts                           â†’ Custom deployment & connection scripts
â”ƒ â”£ ğŸ“œglue_connection.py
â”ƒ â”— ğŸ“œdeploy_gluecustomer360_analytics_workflow.py
â”£ ğŸ“‚.github
â”ƒ â”— ğŸ“‚workflows
â”ƒ   â”— ğŸ“œdeploy.yml                    â†’ GitHub Actions CI/CD deployment workflow
â”£ ğŸ“œrequirements.txt                  â†’ Python dependency definitions
â”£ ğŸ“œsetup.py                          â†’ Build and package ETL as a wheel file
â”— ğŸ“œREADME.md                         â†’ Project documentation and setup info
</code></pre>
## ğŸ”§ Features
- **ETL Pipelines** for:
  - Customer Purchase Behavior
  - Churn Prediction
  - Omni-channel Engagement
  - Fraud Detection
  - Pricing Trends
- **Data Sources**: Amazon RDS (MySQL), Amazon DynamoDB
- **Data Lake Target**: Amazon S3
- **Orchestration**: AWS Glue Workflows, Triggers, and Crawlers
- **Automation**:
  - Infrastructure provisioning via Boto3
  - GitHub Actions for CI/CD deployment.
## ğŸ§± Architecture Overview
          +-------------------+
          |  Amazon RDS       |
          |  Amazon DynamoDB  |
          +--------+----------+
                   |
                   v
          +-------------------+
          |  AWS Glue Jobs    |  (Spark ETL)
          +--------+----------+
                   |
                   v
          +-------------------+
          |   Amazon S3       |
          +-------------------+
                   |
                   v
          +-------------------+
          |  AWS Athena       |  (Query Layer)
          +-------------------+

## ğŸš€ CI/CD Workflow (GitHub Actions)
The deployment pipeline automates:
Building Python wheel for Glue job logic.
Uploading job scripts and artifacts to S3.
Creating/updating Glue jobs, workflows, triggers, and crawlers.
## âš™ï¸ How to Set Up
1. Install Dependencies
   pip install -r requirements.txt
2. Build Python Package
   python setup.py bdist_wheel
3. Upload Job Scripts to S3
Scripts in glue_upload_script/ are synced to the designated S3 bucket folder structure.
## ğŸ” Security
Uses IAM roles with least privilege principle.
CI/CD authentication via GitHub OIDC IAM role for secure access to AWS.
## ğŸ“Œ Best Practices Followed
Modular and reusable ETL code (glue_etl_pipeline/)
Source and deployable code separated
Infra-as-code via Boto3
Parameterized job configurations
Centralized logging and error handling
## ğŸ“ˆ Future Enhancements
Add unit tests for transformations using pytest and moto
Integrate with AWS Glue Data Quality rules
Add performance metrics and CloudWatch alerts
Add versioning to deployment artifacts
Support for streaming ETL jobs (Kinesis, Kafka)
## ğŸ§‘â€ğŸ’» Author
Dipak Vaidya
Senior Software Engineer | Cloud & Data Engineering Enthusiast
<a href="https://bit.ly/2Se2UE7">LinkedIn Profile</a> | <a href="https://bit.ly/2ZaNzWp">GitHub</a>
## ğŸ“„ License
MIT License
Would you like me to help auto-fill your LinkedIn/GitHub or include a custom badge section (like build status, S3 sync success, etc.)?
